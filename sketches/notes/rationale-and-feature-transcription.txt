# Cub: Rationale and Feature Overview

## Introduction and Philosophy

The idea for this monologue is that we can turn it into a structured outline of what Cub is, what the philosophy behind it is, and what its attributes and features are—so that we can have that exist as its own artifact. We can also use it to evaluate the current codebase and what we currently have planned for future iterations and releases.

### The Shift to Code Generation Abundance

To start with a couple of grounding observations: Cub is for people who want to build products using AI to assist in product development, code generation, and release of software.

The big thing underpinning this in terms of new capabilities is the idea that with the current generation of AI models—things like Opus 4.5 and Codex—we continue to get further on the other side of this threshold where you can produce reasonable quality code from very high-level instructions and have that manifest in ways that are reasonably predictable and usable.

This has flipped the idea of code generation being a scarce resource that needs to be very carefully controlled and managed (because it's also incredibly expensive) to a world of **code generation abundance**. Now the issue is not "how do we get this code made?" It's making sure that:
- It's worth making
- It's the right thing
- It works properly
- It fits with what users actually want

This is an inversion from the prior era, which thought of the before-code-generation and after-code-generation phases as being the relatively cheap parts of the process. Now, amongst the backdrop of much more code generation, the bottlenecks really come in the **planning phase** ahead of code generation and in the **assessment phase** after code generation.

### The Steam-to-Electric Analogy

Among other things, this feels like it drives a real realignment of all the tools and processes around code generation. I think that's one of the reasons why we've seen limited success where people want to continue working in existing predominant workflows like lean and agile, with their existing tools that do product management and parceling out of tasks into sprints, trying to make the act of actual code generation as predictable as possible. They either have limited success in adopting code generation tools (maybe some incremental gains, but nothing massive) or outright failures.

If you look at a metaphor of the era of steam power to the era of electric power: in the steam power era, you had the idea of using gravity as an assistant. A lot of the operations of things like drive shafts would happen vertically—the steam would help push a drive shaft up and then gravity release would push it back down again.

With electrical power, it didn't ultimately make sense to just attach an electric motor to the existing steam-oriented machinery. What made more sense was to redesign factories from scratch. So you went from vertical to horizontal factory floors. You went from steam-era factories being located in cities (because that's where the labor was) to electric-era factories being located more in the suburbs—because they needed more space and because they needed fewer people.

Even though there are other factors that happened alongside (like the automobile, the rise of suburbia, etc.), and this is in some ways an oversimplification, I think what we're seeing here is a similar scale of change.

When we look at the other factors that play alongside this code generation revolution, it seems like the scale of change when it comes to making software is as great as steam to electric. **It makes sense to start from first principles and start from scratch** in terms of entire development and product development processes—not just trying to rip out one piece of the process and replace it with this completely new technology.

That in turn requires deep examination of what those first principles are, what makes sense, what different shapes of opportunities look like in this era, how we can take advantage of these tools also in other parts of the process (in planning, ideation, visioning, etc.). It also raises questions that are not unique to product development about the relationship between humans and machines when it comes to creating this work.

Part of the reason why Cub exists as, in a lot of ways, a blank slate when it comes to tool integrations is recognizing that there's a lot we don't know about how all this should work—and that it's a good time to try to compose a new assembly of tools from scratch.

### The Coexistence of Code and Prompts

For the better part of the last hundred years, the bridge between people and computers was code. That could be of different generations—from assembly to higher-level interpreted languages that would in turn be interpreted by other languages. But ultimately, there was some level of an agreed handoff between a set of instructions of what people wanted machines to do and how machines would interpret and execute that—bundled together as "code." Across languages and generations, there are some consistent guiding principles of what code looks like, how it works, and what you can expect a machine to do with it.

Now we have this whole other system alongside: **prompts for LLMs**. I'm using this term incredibly broadly because when you look at the increased tooling and nomenclature of things like skills, commands, and all kinds of things like that, they are ultimately instructions in plain human language—i.e., prompts—that guide an LLM to do some amount of work.

What we're seeing here is an era of coexistence of these things. That feels to me to be medium- to long-term, not just a transitory period where we're phasing out code and replacing it with natural language. Some of the technologies that have arisen in this era, like MCP, are a bridge between code and prompts. There are aspects of this interplay between these two approaches.

A project now is more likely to be more than just what it had been in the past: code, tests, documentation, and maybe some other packaging—where the code was really the central entity that everything else hung off of. We're moving to an era where there's kind of this **dual-centeredness** of projects where prompts are of equal, if not sometimes greater, importance.

That also brings in a logical set of questions about what belongs in a project. What is a project when you think about it in tangible terms from a Git perspective? Instead of your Git project being just the code and maybe the documentation and maybe the tests, it makes sense to bring in holistically all of the things that go into the creation of the software.

These are all artifacts that—whether they are produced by humans (like this conversation) or are produced by machines and can be introspected—belong in a repository alongside the code because they are, in some ways, another expression of the project. Code is just one expression of a project. They are also helpers and hints to figure out what the code was supposed to do if it's not doing exactly what it's supposed to do, or provide ideas for reevaluation if the code is correct but not ultimately desired by users.

### The Anatomy Metaphor

I have this not-super-precise metaphor. When we think about the affordances of code and prompts and other artifacts here, if you think about this from an anatomic perspective:

**Code is the bones of a system.** They are hard and rigid and powerful. They can support weight. But they are also prone to breakage if used in incorrect ways.

**Prompts and the work that happens by way of prompts are more like the muscles.** They can articulate movement. They can provide a tremendous amount of flexibility. But they need to be controlled in very precise ways by some kind of system—otherwise they will create movements that are out of line with what we want.

You could extend this metaphor if you wanted:
- If you're storing other artifacts in a repository, that's literally **the memory**
- You could think of agents as being **the brains** of the system
- You could think of logs and things like that as being **stored energy or fat**

There's more you could do to play around with this, but that feels like one of the questions we're trying to use Cub to explore: at the end of the day, having a holistic understanding of what went into making a project—and not just a single point-in-time set of representations of that as code—will help make better products.

---

## The 15 Attributes of Cub

Now I want to talk about a number of different aspects of what I want Cub to be. These are in some ways user value propositions—if a user of Cub comes along, what could they expect of the system and the way that it works, so that they can take advantage of it in the right ways?

### 1. Reliable and Predictable

Especially in this era of using LLMs and other similar tools to generate software, a lot of the work—particularly in getting the run loop to operate with a relatively tight leash—is about ensuring you can expect the work being requested of the system to be done correctly.

Correctness has a few definitions:
- Code is stable
- Code is either creating or grafting into a clean architecture
- Code is performant
- Code is standards-oriented

These are a lot of the things that are important—essentially good smells of a pretty sophisticated software development lifecycle. We want to make sure that at the unit of a task, the code being produced is correct.

We also want to ensure that **implementations match the task specifications** and the plans and the specs from which they are derived—that there is very little, if any, drift. This is another area where we see challenges with LLMs: both in terms of missing things entirely, and also in making sure it doesn't put in a bunch of placeholders for things that should be implemented as part of that pass, or do things like lower the test coverage percentage to make the tests pass instead of actually doing things the right way.

As a sub-note to this, I think there's some opportunity for intentional drift, where the LLM in the act of actually doing the work realizes a better approach and has a sense of agency, if not recommending that approach back to a human, to implement things in a more streamlined way. We want to separately (we'll talk about this later) have the right kind of observability to ensure that any of those intentional shifts and drifts are understood.

Another key area for inspiring confidence is knowing that when running these things—ultimately at the task level and in a few other stages where we're operating with skipped permission checks—being able to do this work **inside of sandboxes** (whether that's Docker or a self-contained VM) will help insulate somebody from having their machine get blown up as a result of this.

### 2. Economical

Economy hits on a handful of fronts.

**Efficient model usage.** There's a lot out there right now where people provide a tremendous set of instructions and just have a top-shelf model like Opus 4.5 go after everything. That feels wasteful and also probably not likely to be a going concern for very long as the economics of this industry shake out.

A fair amount of the work at a task level can be performed by less sophisticated models—either older models or lower-end models of the current generation like Haiku 4.5 instead of Opus 4.5. There's also an opportunity to use local models in some cases as they improve. We want to make sure that being economical means we can swap out different harnesses and use different models at the task level.

**Budgeting system.** Related to this: these systems can go off and spin their wheels for extended durations. There's a simple budgeting system that helps, particularly for speculative development, to keep the system from either burning a tremendous number of tokens or taking up a lot of time—to stick within some kind of controlled parameters for how much effort to put into something. That budget can be set in a few different ways: either as soft guidance budgets or hard budgets. When something is running, it's not going to create a bunch of surprises.

**Avoiding reinvention.** Other aspects of being economical are about making good decisions in the flow: avoiding reinventing the wheel and creating a lot of code (which can burn tokens but then also create a fair amount of technical debt, as all code is a liability). This means having a preference for finding existing software—either inside the codebase that can be cleanly extended to do something, or finding libraries for that given programming language.

**Avoiding overengineering.** This is where a lot of the good prompting and planning phases come in. If we're moving to a world where code is in abundance, not a scarcity, but all of the reference points we have for generating code are from an era of scarcity—we do want clean architectures, we want things to be able to see around corners a bit, but it does not need to over-engineer either:
- To a perceived potential internet-scale of operations (using message queues for something that's ultimately moving a few messages an hour)
- Creating libraries with 150 different options when only two are going to get used

We want to be smart about how we drive the economics of how this stuff is used.

### 3. Configurable and Extendable

Different people have different workflows—that's an obvious statement. Those workflows can even change from project to project. If you're using some intermediary system like this for an incredibly speculative afternoon spike, you're going to want a different set of affordances than if you're extending production software.

We want:
- **Detailed user and project-level config** for things like workflows, instructions, operating modes, which harness to use, etc.
- **An extensive hook and plug-in system** for custom functionality. If somebody wants to be iMessaged when an epic completes, they should be able to easily drop in a hook or plug-in attached to a hook to do that.
- **Clear separation between true core functionality** (really that skeletal system) from hooks and plug-ins. We can provide default configurations using our own hook and plug-in system. Users can maintain their own set of plug-ins and be able to share and adopt those from some sort of community hub-type resource—which in turn could and probably should work in a way that's relatively decentralized (probably just like Git repos).

From an implementation perspective, I think in this era it's also important to be smart about the fact that a plug-in or hook could be code or could be a skill that in turn is interpreted at runtime by some sort of LLM. We want those implementation differences to not be a major point of complexity or friction.

### 4. Observable

The groundwork for this is that the system is doing a lot to document its work: harness logs, decisions it's making, detailed commits, being able to attach commits back to the files in the work being done and the tasks.

Among other things, it makes it really easy to **trace a feature or a code change from inception to implementation**: through the spec and planning process, through the task staging, through execution and launch, and possibly even rework at some stage of that process.

We want all this information to be easily visible to people in addition to machines:
- Interfaces like the dashboard for being able to see what's going on at any point in time and keeping an eye on the big picture
- Functionality for being able to configure reports and do performant analysis to understand patterns

### 5. Collaborative

Cub is collaborative. It works as a product, project, design, and implementation partner. It finds the right balance of taking direction, also providing guidance, and making good decisions when left on its own.

It really needs to understand the context of the situation and how it should operate within that. If you think about really good human-to-human partnerships, there's an understanding of when one person's driving and the other person isn't, or when it's a true co-creation kind of experience. We want this to not feel like it is purely an implementation machine, nor is it being asked to do things completely autonomously. There's a middle ground and balancing act to make work there.

Also, because it can go off and do things, there's a **consistent interface**—both in the programming architecture parlance and also in a user experience parlance—for raising things to be handled by a human. If it needs API keys that require registration, or it needs a decision to be made between two seemingly good options, there's a logical flow for where that is. That can be something that works in a way that's hook and event driven, with both built-in options and a variety of external plug-ins (for instance, being able to message someone on Telegram or Slack to get their attention to something that needs to be done).

### 6. Composable

A lot of people who are potential users of this, including myself, are folks who are spending a lot of time already with coding agent harnesses like Claude Code, Codex, etc.

This should not feel like an either/or proposition. If you have Cub enabled in a project, you should still be able to use all of the features and functionality of something like Claude Code. What should happen behind the scenes is that through instructions, we should be getting the logs for those sessions, keeping task IDs attached to work that's being done, things like that. Whether you're driving changes to your system directly through something like Claude Code or you're asking Cub to run on its own, you're able to capture this information in a consistent way.

The middle ground between essentially open-ended CLI use and Cub running in an automated fashion is when we're firing up one of these CLIs to conduct interviews. We just want to think about what it looks like for this to operate on a spectrum.

### 7. Proactive

With some sort of **daemon running**, it can figure out how to triage where things stand—different items at different stages. I think that's one of the reasons for modeling the entire stage workflow.

It can figure out if there's an opportunity to expedite something. That could be: completed tasks are in the "needs review" state and Cub can take a first pass at a deep analysis to see if there are any things that need to be fixed. If that's the case and it's logical, just move it back into the staged state for that rework to happen.

If this expediter is running in its own loop from a daemon process, it can also have some sense of where its time and attention can provide the greatest bang for the buck: decreasing the largest unknowns or doing the things that are going to unblock the most work.

It can also be smart about trying to avoid work that would likely conflict with other things that are happening right now, so that we don't have a bunch of merge overhead created by a system being proactive.

### 8. Organized

The system keeps artifacts in consistent formats and puts them in predictable locations so that we don't end up with versions of specs that are all created kind of on their own, littered all over the codebase.

I think there's an opportunity here if we borrow one of the ways that Beads works: there's a branch that exists just for syncing beads issue files, and when you set up Beads in a Git project, it creates a worktree and puts that stuff on the worktree so that all checkouts and clones of a project can have access to an up-to-date single view of what the task list is.

We could potentially look into that for certain artifacts that Cub generates—that we want to be globally aware and globally accessible at the project level, instead of being bound just to a given branch and having to be synchronized at the point of a pull request or something along those lines.

The expectation is that when you have a checkout of a Cub-enabled project that you're working on:
- You can do a variety of things that are local to your branch (about new code, documentation, things that fit with what you're trying to expand)
- But if you're looking to understand the state of a system as a whole, including work that hasn't been done yet (so you can figure out if your feature exploration is actually overlapping with another feature exploration), you have easy access to that project's global data
- Things that are not going to compete with each other from a feature development or bug fixing perspective probably live at that global level, and there's a clean way of keeping that in sync across checkouts

### 9. Comprehensive and Holistic

Cub can assist you with the entire lifecycle of product development from the very beginning of an idea all the way through production and maintenance. That can work across different life stages of a project:
- Starting something new from scratch
- Having Cub come in and get its bearings on a multi-year-old project to be able to confidently ship new features or come up to speed

While it's set up to be able to work across that entire lifecycle, one of the reasons for using a number of artifacts that reflect the different states of a process (even if they're slightly overlapping) is that **you can choose to use Cub for just certain parts of the process**.

- If you have a well-defined product management workflow, as long as you can translate that into tasks and epics, you could use Cub just for run-through-merge-and-release
- You could decide that you really only want to use this to plan work into logical chunks, and then you have a separate system doing the running
- You can do both
- You can have this be something that's very specifically for just one stage of the process

While it provides a logical flow from start to finish, it does not require you to use all of the parts in order to get value from it.

### 10. Vertically Integrated

We're taking a **batteries-included approach** to the code and the prompts that includes things that could otherwise be packaged as, for instance, a separate set of skills.

The idea is that this makes it much easier to be predictable across operations and environments. If a project is being checked out inside a sandbox for a single feature development, you can be more assured that the combination of instructions that a harness is using across a variety of skills and prompts is reliable to that environment—and that it's less likely that some other file somewhere in, say, your home directory is polluting those instructions and creating a slightly different flow for what should be the same work.

This vertical integration is also where, both from a data perspective and from an interface perspective, we see this idea that the **Git repo should be storing all of the knowledge of a project as a contained unit**, not just the code. The code is just one manifestation or expression of an entire project.

### 11. Intuitive

Help and documentation are robust. They remain robust and readable and up to date—both for humans and agents. This includes making sure there are several different cuts through and slices through the system as a whole, so it's also easy for people to do things that are logical extensions (being able to very easily create a custom hook for some type of event that they want to register against, etc.).

For the CLI in particular (and as we develop the web interface as well), the commands, subcommands, switches, and arguments are logical and consistent. We're keeping an eye on this—if this is in a lot of ways the entry point to the system for most users:
- The names of these things are intuitive
- Help descriptions are intuitive
- You're not being asked to provide a task ID one way in one place and a different way in a different place

Also, there are thoughtful fallbacks for places where the syntax on the command line may not be exactly correct, but we know definitively what they mean:
- If somebody says `--version` instead of running the command `version`, or they use the command `help` instead of `--help`, we treat those as synonyms and give them the right thing
- The placement of `--debug` or things like `--verbose` that right now need to come early in a command-line incantation because they're at a global decision level—if they're at the tail end, we should be able to just move that to the right place

We want to make sure as we continue to evolve this system that we pay attention to how people use it and maintain and improve upon it being intuitive for people.

### 12. Self-Learning

Especially since we're doing all of this logging, the system can assess its own work through introspection so that it can find patterns for improvement.

Those could be:
- Recommendations that it makes through this collaborative feedback loop
- Things that it just puts into its own prompt and agent and constitution files itself so that it is less likely to keep making the same mistakes

### 13. Self-Healing

When running, when a run on a task fails, it can assess what the nature of the error was and determine what to do:
- It could have been a transient issue (a network issue or something like that) for which a retry makes sense
- It could be that the model is not capable of doing the work, that there was a mismatch in complexity, and so it can upgrade to a better model on retry
- It can observe itself and see if a task has been running for too long and check in on it or kill the task to try to re-kickstart the process
- It can also assess whether there's an upstream error for some of these types of issues—like, is there a GitHub outage that is causing weird behavior, or some other larger issue that's external to us

### 14. Alignable

Obviously it's working with models that are themselves aligned to the world in sort of global usage. What we want to do is be able to layer on top of that a set of alignments that make sense for different project environments.

For instance, when you're using something like this in a news or journalism context, the **provenance of the data** that you're working with is extremely paramount. So that should de facto ensure that if it's agentically trying to go out and source data:
- It's doing it from reputable sources
- It's checking on those sources
- It's looking for multiple sources of information to be able to validate
- It's keeping track of exactly what it's doing to the data at all steps of the process

Those are things that you shouldn't have to explain in every spec, in every task, in every project—because you're bringing in a set of these contextual alignments into some sort of constitution file that should be easy to set up.

### 15. Easy Things Easy, Hard Things Possible

We're talking about a system that has a lot of sophistication and complexity. It is paramount that we're borrowing a mantra from Perl and Larry Wall: **this will work well if we make the hard things possible while keeping the easy things easy**.

When we think about the design of help menus, the documentation site—yes, we're building all this functionality and it's amazing and powerful, but if we assume some sort of 80/20 usage of this, we're not creating a system that is tremendously complex to be able to just do easy things.

There's some manifestation of this where it's possible you could run `cub run --direct` and just have it do some work. We should update that so that behind the scenes it's creating a task ID if it's not already (I forget what we're doing there). But you shouldn't need to, every single time you want to get something done, go through a four-step planning process.

LLMs help with that a bit because they can be steered to make good assumptions, and sometimes we can skip steps. But we want to make sure that the system sets up each part of the process to pick up where the prior parts left off—and can distinguish between earlier phases of a planning process that were just not done from phases that were knowingly and intentionally skipped or shortcut because they're not super relevant.

If we think about the fact that managing a product involves a tremendous number of heterogeneous inputs—everything from big new feature plans to very specific bug fixes for typos—we want to make sure that we're not falling into a trap of process orientation where a simple typo fix requires 17 steps.

---

Those are all of the attributes that I can think of, and a lot of the grounding principles for this system.
