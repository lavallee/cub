Okay, I want to talk about Cub, what I'm doing here. and the idea for this monologue is that we can turn it into structured outline of what cub is, what the philosophy behind it is, what its attributes and features are so that we can have that exist as its own artifact, and we can also use it to evaluate the current code base and what we currently have planned for future iterations and releases. So I think to start with a couple of rounding observations, Cub is for people who want to build products using AI to the product development, assist in the product development in code generation, and release of software. The big thing underpinning this in the terms of new capabilities is the idea that with the current generation of AI models, things like opus 45 and Codex5.2, that we continue to sort of get further on the other side of this threshold where you can produce reasonable quality code from very high level instructions and have that manifest in ways that are reasonably predictable and usable.  And so that has flipped this idea of code and code to generation being a scarce resource that needs to be very carefully controlled and managed, because it's also incredibly expensive, to a world of code generation abundance And so now, the issue is not, how do we get this code made? It's making sure that it's worth making that it's the right thing in that it works properly and that it fits with what users actually want. So it's kind of this inversion in between the prior era, which really thought of the before code generation and after code generation as being the relatively cheap parts of the process to a world now where amongst the backdrop of a lot more code generation, we have the bottlenecks really coming in the planning phase ahead of code generation and in the assessment phase after code generation.  So among other things, this feels like it drives a real alignment of all of the tools and processes that are around co-generation. And I think that's one of the reasons why we've seen limited success where people want to continue to work in the existing predominant workflows, like lean and agile, and what their existing tools that do a lot of product management and parceling out of tasks and into sprints and try to essentially make the act of actual cogeneration as predictable as possible, sort of have either limited success in adopting code generation tools that provide maybe some infremental gains, but not anything massive, as well as a lot of failures. If you look at a metaphor of the era of steam power to the era of electric power, you had, in the steam power era, the idea of using gravity as an assistant. So a lot of the operations of things like drive shafts would happen vertically. It would, it would basically the steam would help push a drive shaft up and then gravity release would push it back down again.  And with electrical power, it didn't ultimately make sense to just try to attach an electric motor to the existing steam oriented machinery. What made more sense was to redesign factories from scratch. And so you went from vertical to horizontal factory floors.  You went from these steam era factories being located in cities because that's where the labor was to machine electric era factories being located more in the suburbs, A, because they needed more space and B, because they needed fewer people. And so even though there are other factors that happen alongside, like the automobile, the rise of suburbia, things like that, and this is in some ways an oversimplification. I think what we're seeing here is a similar scale of change.  And so when we look at the other factors that play alongside this code generation, it seems like the scale of change that we're talking about when it comes to making software is as great as steam to electric. And so it makes sense to start from first principles and start from scratch in terms of the entire, different flows of entire development, product development processes, not just trying to rip up one piece of the process out and replace it with this new, completely new technology. That in turn requires deep examination of what those first principles are, what makes sense, what different shapes of opportunities look like in this era, how we can take advantage of these tools also in other parts of the process and the planning process and in the ideation of visioning, etc.  And also, it asks a lot of questions that are not unique to product development about the relationship between humans and machines when it comes to creating this work. And so part of the reason why Cub exists as in a lot of ways, a blank slate when it comes to tool integrations is recognizing that there's a lot that we don't know about how all this should work and that it's a good time to try to compose a new assembly of tools from scratch. So that's one big area and one big question.  The next one is that for the better part of the last hundred years, the bridge between people and computers was code, and that could be of different generations from assembly to higher level interpreted languages, that would then in turn be interpreted by other languages. But ultimately, there was some level of an agreed handoff between a set of instructions of what people wanted machines to do and how machines would interpret and execute that and that sort of in a large bundle as code. And across languages and generations, there are some consistent guiding principles of what code looks like, how it works, what you can expect a machine to do with it, et cetera.  And now we have this whole other system alongside in terms of prompts for LLMs. And I'm using this term incredibly broadly because when you look at the increased tooling and nomenclature of things like skills and commands and all kinds of things like that, they are ultimately instructions in plain human language, i.e. prompts that guide an LLM to do se amount of work. And so what we're seeing here is an era of coexistence of these things.  And that feels to me to be medium to long term, not just this kind of transitory period where we're phasing out code and replacing it with natural language. Some of the technologies that we've seen arise in this era, like MCP are a bridge between code and prompts. And so I think that there are, you know, some aspects of this interplay between these two approaches, A project now is more likely to be more than just what had been in the past, right?  Of code and tests and documentation and maybe some other packaging for that where the code was really the central entity that everything else hung off of to an era where there's kind of this dualcenteredness of projects where prompts are of equal, if not sometimes greater importance. And that also brings in, I think, a logical set of questions about what belongs in a project, what a project is when you think about it inangible terms from like a get perspective that instead of your your, you know, your get project being just the code and maybe the documentation and maybe the tests and things like that, that it makes sense to bring holistically in all of the things that go into the creation of the software, right? These are all artifacts that whether they are produced by humans like this conversation or are produced by machines and can be introspected, that belong in a repository alongside the code because they are in some ways, another expression of the project code is just one expression of a project.  They are also helpers and hints to figure out what the code was supposed to do if it's not doing exactly what it's supposed to do or provide ideas for reevaluation if the code is correct, but it's not ultimately desired by users and things like that. So I have this not super precise metaphor. When we think about the affordances of code and prompts and other artifacts here that if you think about this from an anatomic perspective, code really the bones of a system, right?  That they are hard and rigid and powerful and that they can support weight and things like that. But they are also prone to brickage if used in incorrect ways. And prompts and the work that happens by way of prompts are more like the muscles that they can articulate movement, that they can provide a tremendous amount of flexibility, but that they need to be controlled in very precise ways by some kind of system.  Otherwise they will create movements that are out of line with what it is that we want. You could extend this metaphor if you wanted to, if you are destroring other artifacts in a repository that are literally the memory. You could think of agents as being the brains of the system.  You could think of logs and things like that, being stored energy or fat. You know, there's more that you could do to play around with this, but that feels like one of the questions that we're trying to use Cub to explore as well is that at the end of the day, having a holistic understanding of what went into making a project and not just a single point in time set of representations of that as code will help make better products. So that's that piece of it.  I'm going to pause here and go on to the next piece



Okay, so now I want to talk about uh a number of different aspects of what I want cub to be. These are in some ways user value propositions, right? If a user of cub comes along, what could they expect of the system and the way that it works?  So that they have the ability to take advantage of it in the right ways. So one thing is that it should be reliable and predictable, right? Especially in this era of using LLMs and other similar tools to generate software.  You know, I think that a lot of the work, particularly in getting the run loop to operate with a relatively tight leash is that you can expect the work that's being requested of the system to be done correctly and correctness has a few definitions that that code is stable, that it is either creating or grafting into a clean architecture. The code is performance, that it's standards oriented, right? These are a lot of these things that are important essentially like good smells of a, you know, pretty sophisticated software development life cycle.  And we want to make sure that at that unit of a task, that the code being produced is correct. We also want to ensure that the implementations match the task specifications and the plans and the specs from which they are derived, that there is very little, if any, drift, that is another area that I think we see where we see challenges with LLMs that is both in terms of missing things entirely. That is also in making sure that it doesn't put in a bunch of placeholders for things that should be implemented as part of that pass or do things like lower the test coverage percentage to make the tests pass instead of actually doing things the right way.  There is, as a sub note to this, I think some opportunity for drift, where the LLM and the act of actually doing the work realizes a better approach and has a sense of agency, if not recommending that approach back to a human to implement things in a more streamlined way. I think we want to separately, and we'll talk about this later, have the right kind of observability to ensure that any of those intentional shifts and drifts are understood. Another key area of being able to inspire confidence in this first bullet point is knowing that in running these things, ultimately at the task level and in a few other stages, where, you, we're operating with where we're skipping permission, checks, and things like that, that being able to do this work inside of sandboxes, whether that's Dcker or a self-contained VM or things like that will help insulate somebody from having their machine get blown up as a result of this.  So that's number one. Number two is that the system is economical and economy hits on a handful of fronts. You know, One is that it's using models efficiently.  There's a lot out there right now where people provide a tremendous set of instructions and just have a top shelf model like Opus 45, go after everything. That feels wasteful and also probably not likely to be a going concern for very long as the economics of this industry shake out. But that a fair amount of the work at a task level can be performed by less sophisticated models, either older models or sort of lower end models of the current generation like Haiku 45 instead of Opus 45, there's also an opportunity to use.  In some cases, local models, as they improve and things like that. So we want to make sure that being economical means that we can swap out different harnesses and use different models at the task level. Related to this in that these systems can go off and spend their wheels in a for extended durations that there's a simple budgeting system that helps particularly for speculative development to be able to keep the system from either burning a tremendous number of tokens or taking up a lot of time to stick within some kind of controlled parameters for how much effort to put into something.  So that is a way where that budget can be set in a few different ways and those can be either soft guidance budgets or hard budgets, but that when something is running, it's not going to create a bunch of surprises. There are other aspects of being economical that are about making good decisions in the in the flow so that is, you know, avoiding reinventing the wheel and creating a lot of code and, which can burn tokens, but then also create a fair amount of technical debt, as all code is a liability. And that is by having a preference for finding existing software, either inside the code base, that can be cleanly extended to do something or finding things that are libraries for that given programming language or things along those lines.  And then similarly, and this is where I think a lot of the good prompting and planning phases come in is avoiding overengineering, right? That if we were moving to this world where code is in abundance, not a scarcity, but all of the reference points we have for generating code are a code from an era of scarcity. We do want clean architectures.  We want things to be, we want the system to be able to see around corners a bit, but it does not need to oveengineer either to a perceived potential internet scale of operations where it's using messaging cues and things like that for something that's ultimately moving through messages an hour. Nor is it creating libraries with 150 different options when only two are going to get used. And so I think we want to be smart about how we drive the economics of how this stuff is used.  Number three is that the system is quite configurable and extendable. So different people have different workflows. That's an obvious statement.  And those workflows can even change from project to project. If you're trying to use some intermediary system like this for some incredibly speculative afternoon, Spike, you're going to want a different set of affordances than if you are extending production software. And so we want detailed user and project level config for things like workflows, instructions, operating modes, et cet.  Even things like, which harness to use and things like that, as well as an extensive hook and plug-in system for custom functionality. So if somebody wants to be iMessaged when an epic completes, they should be able to easily drop in a hook, a plug-in attached to a hook to be able to do that. We want a clear separation between true core functionality.  Really that skeletal system from hooks and plug-ins. And that can be things that we are where we are providing default configurations and we're using our own hook and plug-in system to do that. That can also be being able to maintain your own set of plug-ins and be able to share and adopt those from some sort of community hub type resource that in turn could and probably should work in a way that's relatively decentralized or at least decentralized a bowl, so that could probably just be like Gitripos and things like that.  As we look at what a plug-in or hook could be from an implementation perspective, I think in this era, it's also important to be smart about the fact that that could be code or that could be a skill that in turn is interpreted at runtime by some sort of LLM. And so I think we want to be able to have that those implementation differences not be a major either point of complexity or friction. Number four is that we want the system to be observable, right?  And that the groundwork for that is that the system is doing a lot to document its work. So harness logs, decisions it's making, you know, teeth field commits and being able to attach commits back to the files in the work that's being done in the tasks. Right.  And so that, among other things, it makes it really easy to be able to trace a feature or a code change from inception to implementation through the spec and planning process, through the task staging, and through execution in launch, and possibly even rework. at some stage of that that process. And we want that to be all this information, to be easily visible to people in addition to machines, right, so that there are interfaces like the dashboard first being able to see what's going on at any point in time and being able to keep an eye on the big picture in that there's a set of functionality for being able to configure reports and be able to do performant analysis to be able to understand patterns. Number five is that the system is collaborative.  Cub is collaborative. It works as a product project design implementation partner. It finds the right balance of taking direction, also providing guidance and making good decisions when left on its own.  And so it really needs to understand the context of the situation and sort of how it how it should operate within that, right? Those if you think about really good human to human partnerships, there's an understanding of when one person's driving and the other person isn't or when it's a true co-creation kind of experience. And so we want this to not feel like it is purely a implementation machine, nor is it being asked to do things completely autonomously, right?  So there's a there's kind of a middle ground and balancing act to make work there. It also, in being collaborative because it can go off and do things that there's a consistent interface, both in the programming architecture parlance and also in a user experienced parlance for raising things to be handled by a human. So if it means API keys that require registration or it needs a decision to be made between two seemingly good options, that there's a logical flow for where that is.  That can be something that works in a way that's that's by being hook and event driven. with both built-in options and a variety of external plug-ins. So it could, for instance, be able to message someone on telegram or Slack to be able to get their attention to something that needs to be done. Number six is that is the system is composable and by that, I mean that, you know, I think a lot of people who are potential users of this, including myself, are folks who are spending a lot of time already with coding agent harnesses, like clod code, Codex, etc.  And so this should not feel like an either or proposition. You should be able to, if you have C enabled in a project, you should still be able to use all of the features and functionality of something like Claod code. What should happen behind the scenes a little bit is that through instructions, we should be getting the logs for those sessions.  We should be, you know, keeping task IDs attached to work that's being done, things like that. So that whether you're driving changes to your system through directly through something like Claude or your asking cub to run on its own, that you are, you're able to capture this information in a consistent way. The middle ground, I think, between essentially, you know, sort of open-ended CLI use and cub running in an automated fashion is when we are firing up one of these CLIs to conduct interviews.  And so I think we just want to think about what it looks like for this to operate on a spectrum. Number seven is that the system is proactive, right? So I think with some sort of demon running, it can figure out how to triage where things stand at each different items at different stages.  I think that's one of the reasons for modeling the entire stage workflow. and figure out if there's an opportunity for it to expedite something so that could be that I completed tasks are in the needs review state and Cub can take a first pass at a deep analysis to see if there are any things that need to be fixed. And if that is the case and it's logical, just move it back into the staged state for that rework to happen. I think it can also look at, it can, if it's running, if this expediter is running in its own loop from a demon process, it can also have some sense of where it's time and attention can, can provide the greatest bang for the V decreasing the largest unknowns or doing the things that are going to unblock the most work.  It can also be smart about trying to avoid work that would likely conflict with other things that are happening right now. so that we don't have a bunch of merge overhead created by a system being proactive. Number eight is that the system is organized, that it keeps artifacts and consist formats, and it puts them in predictable notations so that we don't end up with versions of specs that all are created kind of on their own, liter all over the code base and things along those lines. I think there's an opportunity here.  If we borrow one of the ways that Beads works where there's a global, there's a branch that exists just for sinking beads issue files and that when you set up beads in a get in a project, it creates a work tree and puts that stuff on the work tree so that all checkouts and clones of a project can have access to an up-to-date single view of what the task list is. We could potentially look into that for certain artifacts, that Cub generates. that we want to be globally aware and globally accessible at the project level, instead of being sort of bound just to a given branch and having to be synchronized at the point of a pull cluster or something along those lines. But the expectation is that when you have a checkout of Cub that you're working on, that you are able to do a variety of things that are local to your branch, that are about new code, documentation, things like that, that sort of fit with the, uh, the way that you were trying to expand the system, but that if you are looking to understand the state of a system as a whole, including work that hasn't been done yet, so you can figure out if you're feature exploration is actually overlapping with another feature exploration that you have easy access to the, to that project's global data and that things that are not going to compete with each other from a feature development or bug fixing perspective, probably live at that global level.  And there's a clean way of keeping that in sync across across checkouts. Okay, so that was number eight. Number nine is that the system is comprehensive and holistic.  So what this means is that Cub can assist you with the entire life cycle of product development from the very beginning of an idea all the way through production and maintenance, and then that can work across different life stages of a project. So that could be starting something new from scraps, that could be having Cub come in and get its bearings on a multi-year old project to be able to confidently ship new features or come up to speed. And that while it is set up to be able to work across that entire life cycle, one of the reasons for using these, you know, a number of artifacts that reflect the different states of a process, even if they are slightly overlapping, is that you can choose to use Cub for just certain parts of the process.  So if you have a whole variety of, if you have a well-defed products management workflow, as long as you can translate that into tasks in epics, you could use Cub run. through merge. and release. You could decide that you really only want to use this to be able to plan work into logical trunks, and then you have a separate system that's doing the running. You can do both, you could do, you know, you can have this be something that is very specifically for just one stage of the process.  But that, while it provides a logical flow from start to finish, it does not require you to have to use all of the parts in order to get value from it. Number 10 is that it's vertically integrated, right? So we're taking a batte's included approach to the code and the prompts that includes things that could otherwise be packaged. as, for instance, a separate set of skills or things like that.  The idea, there's that, that makes it much easier to be predictable across operations and environments, so that if a project is being checked out inside a sandbox for a single feature development that you can be more assured that the combination of instructions that a harness is using across a variety of skills and proms is reliable to that environment in that it's less likely that some other file, somewhere in, say, your home directory is polluting those instructions and creating a slightly different flow for what should be the same work. This vertical integration is also where, both from us, a data perspective and from an interface perspective, I think we see this idea that the get Repo should be storing the knowledge of a project as, as all of the knowledge of a project as it contained unit, not just the code, right, that the code is just one manifestation of expression of an entire project. Number 11 is that the system is intuitive, that help in documentation are robust.  They remain robust and readable and up to date. And that is both for humans and agents. And that is making sure, among other things, that there are several different cuts through and slices through the system as a whole.  So it's also easy for people to do things that are logical extensions, right, being able to very, very easily create a custom hook for some type of event that they want to register against and things like that. The CLI and then, you know, I think as we were developing the web interface as well, for the CLI in particular, the commands, subcommands, switches and arguments are logical and consistent. And that we are keeping an eye on that right, if this is in a lot of ways, the entry point to the system for most users, that it is that the names of these things are intuitive, that help descriptions are intuitive, that you're not being asked to provide a task ID one way and one place in a different way and a different place.  And also that there are thoughtful fallbacks for sort of places where the syntax on the command line may not be exactly correct, but we know definitively what they mean. So if somebody says dash dash version, instead of running the command version, or they say they use the command help instead of dash dash help that we are able to treat those as synonyms and give them the right thing. Another concrete example here is the placement of dash dash debug or things like dash dash for Bos, that right now, I think need to come early in a command line incantation because they are at a sort of a global decision level.  I think if we see that, we can just, if it's at the tail end, we should be able to just move that to the right place. So I want to make sure as we are continuing to evolve this system itself, that we are paying attention to how people use it and that we maintain and improve upon it being intuitive for people. 12 is that the system is self-learning. And by that means by that I mean that it can, especially since we're doing all of this logging, that it in assess its own work through introspection, so that it can find patterns for improvement.  Those could be recommendations that it makes through this collaborative sort of feedback loop. It could be things that it just puts into its own prompt and agent and constitution files itself so that it is less likely to keep making the same mistakes. Number 13 is that it is self- healing when it's running.  So when a run on a task fails, it can has one example. It can assess what the nature of the error was and determine what to do. So it could have been a transient issue, a network issue, or something like that, for which a retry makes sense.  It could be that the bottle is not capable of being able to do the work, that there was a mismatch and complexity, and so it can upgrade the to a better model on retry. It can observe itself and see if a task has been running for too long and, you know, check in on it or kill the task to try to re-kickstart the process. You can also assess whether there's an upstream error for some of these types of issues.  Like, is there a GitHub outage that is causing weird behavior or you know, some some other sort of like larger issue that's external to us. Number 14 is that the system is alignable and by that, I mean, obviously it's working with a, with models that are themselves aligned to the world in sort of global usage. What we want to do is be able to layer on top of that a set of alignments that make sense for different project environments.  For instance, when you're using something like this in a news or journalism context, the provenance of the data that you're working with is extremely paramount. And so that should de facto ensure that if it is agentically trying to go out and source data that it's doing it from reputable sources, that it's checking on those sources, that it's looking for multiple sources of information to be able to validate, and things like that, that it is keeping track of exactly what it's doing to the data at all steps of the process, that those are things that you shouldn't have to explain every in every spec in every task, in every, every project, because you're bringing in a set of sort of these contextual alignments into some sort of constitution file that should be should be easy to set up. And then finally, for number 15, obviously, we are talking about a system that, has a lot of sophistication and complexity.  And it is paramount that we were a borrowing a mantra from Pearl in Larry Wall, that this will work well if we make the hard things possible while keeping the easy things easy. And so when we think about the design of help menus, when we think about, or the documentation site, you know, if while yes, we are building all this functionality and it's amazing and powerful and things like that, if we, you know, assume that some sort of 80-20, you know, usage of this, that we're not creating a system that is tremendously complex to be able to just do easy things, right? So there's some manifestation of this where it is possible you could run, you could do cub run dash dash direct and just have it do some work.  We should update that so that it behind the scenes, it's creating a task ID, if it's not already, I forget what we're doing there. But that, you know, you shouldn't need to every single time you want to get something done, go through a four step planning process. LLMs help with that a bit because they can be steered to make good assumptions and sometimes we can skip steps and things along those lines.  But we want to make sure that the system sets each part of the process kind of picks up where the prior parts left off can distinguish between earlier phases of a planning process that were just not done from phases that were knowingly and intentionally skipped or shortcut it a bit because they're not super relevant. I think if one of the things that we can do with this, as we think about how to make, if we think about the facts that managing a product involves a tremendous number of heterogenous inputs, everything from, you know, big, you, new feature plans to very specific bug fixes for typos. We want to make sure that we're not following into a trap of process orientation where a simple typo fix requires 17 steps.  And so let's make sure that we're doing that as well. So those are all of the attributes that I can think of in a lot of the grounding principles for this system.