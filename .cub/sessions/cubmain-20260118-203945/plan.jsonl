{"id": "cub-E01", "title": "Phase 1: Foundation - Establish Stability Framework", "description": "## Context\nThis epic establishes the foundation for the test coverage improvement initiative. It creates the stability tracking system and achieves the MVP target of 60% coverage on cli/run.py.\n\n## Goal\nCreate .cub/STABILITY.md, update CLAUDE.md, configure per-file coverage thresholds, and write comprehensive tests for cli/run.py.\n\n## Success Criteria\n- [ ] STABILITY.md exists with tier definitions\n- [ ] CLAUDE.md references STABILITY.md\n- [ ] pyproject.toml has per-file coverage config\n- [ ] cli/run.py coverage >= 60%\n- [ ] CI passes with new thresholds", "status": "open", "priority": 0, "issue_type": "epic", "labels": ["phase-1", "model:sonnet", "complexity:high"]}
{"id": "cub-001", "title": "Create STABILITY.md with tier definitions", "description": "## Context\nThe stability tracking system is the key innovation in this testing initiative. It communicates module confidence levels to both human developers and AI agents, ensuring everyone knows which code is battle-tested vs. experimental.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-4 hours\n**Approach:** Create .cub/STABILITY.md following the structure defined in the architecture. Analyze current coverage data to assign modules to appropriate tiers.\n\n## Implementation Steps\n1. Create `.cub/STABILITY.md` file\n2. Define four tiers: Solid (80%+), Moderate (60%+), Experimental (40%+), Untested (no threshold)\n3. Assign all modules to appropriate tiers based on current coverage analysis\n4. Add guidance for agents making changes to each tier\n5. Include rationale for each tier assignment\n\n## Acceptance Criteria\n- [ ] `.cub/STABILITY.md` exists and is well-formatted\n- [ ] All Python modules in src/cub/ are assigned to a tier\n- [ ] Tier criteria are clearly documented\n- [ ] Guidance for AI agents is included\n- [ ] File follows markdown best practices\n\n## Files Likely Involved\n- `.cub/STABILITY.md` (new)\n\n## Notes\nRefer to triage report for current coverage numbers per module. The existing well-tested modules (config/loader, harness/backend, tasks/backend) go in Solid tier.", "status": "open", "priority": 0, "issue_type": "task", "labels": ["phase-1", "model:sonnet", "complexity:medium", "docs", "setup"], "dependencies": [{"depends_on_id": "cub-E01", "type": "parent-child"}]}
{"id": "cub-002", "title": "Update CLAUDE.md to reference STABILITY.md", "description": "## Context\nFor the stability system to be effective, AI agents need to discover it. CLAUDE.md is read by Claude Code at session start, making it the ideal place to reference STABILITY.md.\n\n## Implementation Hints\n\n**Recommended Model:** haiku\n**Estimated Duration:** 30 minutes - 1 hour\n**Approach:** Add a section to CLAUDE.md that explains the stability system and tells agents to check STABILITY.md before modifying code.\n\n## Implementation Steps\n1. Read current CLAUDE.md to understand structure\n2. Add a 'Module Stability' section explaining the tier system\n3. Include instruction to check .cub/STABILITY.md before making changes\n4. Explain what each tier means for testing requirements\n5. Update AGENTS.md symlink if needed\n\n## Acceptance Criteria\n- [ ] CLAUDE.md contains reference to STABILITY.md\n- [ ] Instructions explain how agents should use stability info\n- [ ] Testing requirements per tier are mentioned\n- [ ] Symlink to AGENTS.md still works\n\n## Files Likely Involved\n- `CLAUDE.md`\n- `AGENTS.md` (symlink)\n\n## Notes\nKeep additions concise - agents have limited context. Focus on actionable guidance.", "status": "open", "priority": 0, "issue_type": "task", "labels": ["phase-1", "model:haiku", "complexity:low", "docs"], "dependencies": [{"depends_on_id": "cub-E01", "type": "parent-child"}, {"depends_on_id": "cub-001", "type": "blocks"}]}
{"id": "cub-003", "title": "Configure per-file coverage thresholds in pyproject.toml", "description": "## Context\nCoverage thresholds enforce the stability tier system at CI time. Different modules have different coverage requirements based on their tier.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-3 hours\n**Approach:** Use coverage.py's per-file configuration to set minimum thresholds. Start with key modules in each tier.\n\n## Implementation Steps\n1. Research coverage.py per-file threshold syntax\n2. Add [tool.coverage.report] section to pyproject.toml\n3. Configure thresholds for Solid tier modules (80%)\n4. Configure thresholds for Moderate tier modules (60%)\n5. Configure thresholds for Experimental tier modules (40%)\n6. Exclude Untested tier modules from threshold enforcement\n7. Test locally that coverage report respects new config\n\n## Acceptance Criteria\n- [ ] pyproject.toml has coverage configuration\n- [ ] Solid tier modules require 80% coverage\n- [ ] Moderate tier modules require 60% coverage\n- [ ] Experimental tier modules require 40% coverage\n- [ ] Untested tier modules are excluded from thresholds\n- [ ] `pytest --cov` respects new configuration\n\n## Files Likely Involved\n- `pyproject.toml`\n- `.coveragerc` (if separate config needed)\n\n## Notes\ncoverage.py supports `[report] fail_under` but per-file thresholds may need `.coveragerc` or a custom script. Research the best approach.", "status": "open", "priority": 0, "issue_type": "task", "labels": ["phase-1", "model:sonnet", "complexity:medium", "setup"], "dependencies": [{"depends_on_id": "cub-E01", "type": "parent-child"}, {"depends_on_id": "cub-001", "type": "blocks"}]}
{"id": "cub-004", "title": "Write comprehensive tests for cli/run.py core loop", "description": "## Context\ncli/run.py is the core execution loop - the heart of cub. It currently has only 14% coverage despite being the most critical module. This task brings it to 60%+ with meaningful tests.\n\n## Implementation Hints\n\n**Recommended Model:** opus\n**Estimated Duration:** 4-8 hours (half to full day)\n**Approach:** Study the existing test_run_direct.py for patterns. Focus on testing pure logic first (prompt generation, task selection), then test harness invocation with mocked subprocess.\n\n## Implementation Steps\n1. Analyze cli/run.py to identify testable functions:\n   - generate_system_prompt()\n   - generate_task_prompt()\n   - Task selection/filtering logic\n   - Signal handling\n   - Exit conditions\n   - Budget tracking\n2. Review existing test_run_direct.py for patterns to follow\n3. Create/expand test_cli_run.py with comprehensive tests\n4. Mock subprocess calls for harness invocation tests\n5. Test error handling paths\n6. Test CLI flags and their interactions\n7. Verify coverage reaches 60%+\n\n## Acceptance Criteria\n- [ ] cli/run.py coverage >= 60%\n- [ ] Tests cover prompt generation functions\n- [ ] Tests cover task selection logic\n- [ ] Tests cover signal handling (SIGINT)\n- [ ] Tests cover exit conditions\n- [ ] Tests mock harness subprocess calls appropriately\n- [ ] All new tests pass\n- [ ] Tests follow existing patterns in conftest.py\n\n## Files Likely Involved\n- `tests/test_cli_run.py` (new or expand)\n- `tests/test_run_direct.py` (reference)\n- `src/cub/cli/run.py`\n- `tests/conftest.py` (add fixtures if needed)\n\n## Notes\nThis is the most important task in Phase 1. cli/run.py has 660 missing lines. Focus on the most critical paths first:\n1. Main loop execution flow\n2. Task-to-prompt conversion\n3. Harness result handling\n4. Clean exit scenarios", "status": "open", "priority": 0, "issue_type": "task", "labels": ["phase-1", "model:opus", "complexity:high", "test", "critical-path"], "dependencies": [{"depends_on_id": "cub-E01", "type": "parent-child"}]}
{"id": "cub-005", "title": "Verify CI passes with new coverage thresholds", "description": "## Context\nAfter adding tests and configuring thresholds, we need to ensure CI passes reliably. This validates the Phase 1 MVP.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 1-2 hours\n**Approach:** Run full test suite locally, push changes, verify GitHub Actions pass. Fix any issues.\n\n## Implementation Steps\n1. Run `pytest --cov=src/cub --cov-report=term-missing` locally\n2. Verify cli/run.py shows >= 60% coverage\n3. Check all tier thresholds are respected\n4. Push changes to trigger CI\n5. Monitor GitHub Actions for failures\n6. Fix any issues that arise\n7. Ensure cross-platform tests pass (Ubuntu + macOS)\n\n## Acceptance Criteria\n- [ ] Local pytest passes with coverage thresholds\n- [ ] GitHub Actions CI passes\n- [ ] Coverage uploads to Codecov successfully\n- [ ] No flaky test failures\n- [ ] Both Ubuntu and macOS runners pass\n\n## Files Likely Involved\n- `.github/workflows/test.yml`\n- All test files\n\n## Notes\nThis is the Phase 1 checkpoint. If CI passes, the MVP is complete.", "status": "open", "priority": 0, "issue_type": "task", "labels": ["phase-1", "model:sonnet", "complexity:medium", "checkpoint"], "dependencies": [{"depends_on_id": "cub-E01", "type": "parent-child"}, {"depends_on_id": "cub-003", "type": "blocks"}, {"depends_on_id": "cub-004", "type": "blocks"}]}
{"id": "cub-E02", "title": "Phase 2: Coverage Expansion - Critical Modules and CI Improvements", "description": "## Context\nWith the foundation in place, this phase expands coverage to other critical modules and adds CI improvements for faster feedback.\n\n## Goal\nAdd tests for tasks/service.py, pr/service.py, harness/codex.py. Create contract tests. Enable parallel test execution.\n\n## Success Criteria\n- [ ] core/tasks/service.py coverage >= 60%\n- [ ] core/pr/service.py coverage >= 60%\n- [ ] core/harness/codex.py coverage >= 60%\n- [ ] Contract tests exist for harness CLIs\n- [ ] pytest-xdist enabled in CI", "status": "open", "priority": 1, "issue_type": "epic", "labels": ["phase-2", "model:sonnet", "complexity:high"], "dependencies": [{"depends_on_id": "cub-E01", "type": "blocks"}]}
{"id": "cub-006", "title": "Add tests for core/tasks/service.py", "description": "## Context\ncore/tasks/service.py handles the task lifecycle - finding ready tasks, updating status, managing dependencies. It's at 38% coverage and critical for cub's operation.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 3-4 hours\n**Approach:** Test the TaskService class methods with mocked backend. Focus on task selection algorithm and status transitions.\n\n## Implementation Steps\n1. Analyze core/tasks/service.py to identify testable methods\n2. Create tests/test_tasks_service.py (or expand existing)\n3. Test find_ready_task() with various task states\n4. Test update_task_status() transitions\n5. Test dependency resolution logic\n6. Test error handling paths\n7. Verify coverage reaches 60%+\n\n## Acceptance Criteria\n- [ ] core/tasks/service.py coverage >= 60%\n- [ ] Tests cover task selection algorithm\n- [ ] Tests cover status transitions\n- [ ] Tests cover dependency handling\n- [ ] Tests mock backend appropriately\n- [ ] All tests pass\n\n## Files Likely Involved\n- `tests/test_tasks_service.py` (new or expand)\n- `src/cub/core/tasks/service.py`\n- `tests/conftest.py`\n\n## Notes\nThe task service is the bridge between CLI and backends. Good test coverage here prevents regressions in task management.", "status": "open", "priority": 1, "issue_type": "task", "labels": ["phase-2", "model:sonnet", "complexity:medium", "test"], "dependencies": [{"depends_on_id": "cub-E02", "type": "parent-child"}]}
{"id": "cub-007", "title": "Add tests for core/pr/service.py", "description": "## Context\ncore/pr/service.py handles PR creation logic. It's at 39% coverage and involves GitHub API interactions that need careful mocking.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 3-4 hours\n**Approach:** Mock gh CLI calls. Test PR body generation, branch handling, and error cases.\n\n## Implementation Steps\n1. Analyze core/pr/service.py for testable methods\n2. Create/expand tests/test_pr_service.py\n3. Test PR creation flow with mocked gh CLI\n4. Test PR body generation from tasks\n5. Test branch validation logic\n6. Test error handling (API failures, auth issues)\n7. Verify coverage reaches 60%+\n\n## Acceptance Criteria\n- [ ] core/pr/service.py coverage >= 60%\n- [ ] Tests cover PR creation flow\n- [ ] Tests cover body generation\n- [ ] Tests cover error scenarios\n- [ ] gh CLI calls are mocked appropriately\n- [ ] All tests pass\n\n## Files Likely Involved\n- `tests/test_pr_service.py`\n- `src/cub/core/pr/service.py`\n- `tests/conftest.py`\n\n## Notes\nPR creation involves multiple external calls (git, gh). Use fixtures to mock subprocess consistently.", "status": "open", "priority": 1, "issue_type": "task", "labels": ["phase-2", "model:sonnet", "complexity:medium", "test"], "dependencies": [{"depends_on_id": "cub-E02", "type": "parent-child"}]}
{"id": "cub-008", "title": "Add tests for core/harness/codex.py", "description": "## Context\ncore/harness/codex.py implements the Codex harness. It's at 14% coverage and follows similar patterns to the Claude harness.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-3 hours\n**Approach:** Reference test_harness_claude.py for patterns. Mock subprocess for codex CLI calls.\n\n## Implementation Steps\n1. Review test_harness_claude.py for patterns\n2. Create tests/test_harness_codex.py\n3. Test codex harness initialization\n4. Test command building with various options\n5. Test result parsing\n6. Test error handling\n7. Verify coverage reaches 60%+\n\n## Acceptance Criteria\n- [ ] core/harness/codex.py coverage >= 60%\n- [ ] Tests follow claude harness test patterns\n- [ ] Command building is tested\n- [ ] Result parsing is tested\n- [ ] Error handling is tested\n- [ ] All tests pass\n\n## Files Likely Involved\n- `tests/test_harness_codex.py` (new)\n- `tests/test_harness_claude.py` (reference)\n- `src/cub/core/harness/codex.py`\n\n## Notes\nCodex harness is similar to Claude harness. Reuse fixture patterns where possible.", "status": "open", "priority": 1, "issue_type": "task", "labels": ["phase-2", "model:sonnet", "complexity:medium", "test"], "dependencies": [{"depends_on_id": "cub-E02", "type": "parent-child"}]}
{"id": "cub-009", "title": "Create contract tests for harness CLIs", "description": "## Context\nContract tests verify that external CLI tools behave as our code expects. They catch breaking changes in upstream tools before they cause production issues.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 3-4 hours\n**Approach:** Create tests/contracts/ directory. Write tests that verify CLI version output format, help text, and key command structures.\n\n## Implementation Steps\n1. Create tests/contracts/ directory\n2. Add pytest marker @pytest.mark.contract\n3. Create test_harness_contracts.py\n4. Test `claude --version` output format\n5. Test `codex --version` output format\n6. Test `bd --version` output format\n7. Test `gh --version` output format\n8. Add skip logic if CLI not installed\n9. Document contract tests in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] tests/contracts/ directory exists\n- [ ] Contract tests for claude, codex, bd, gh CLIs\n- [ ] Tests skip gracefully if CLI not installed\n- [ ] @pytest.mark.contract marker is registered\n- [ ] Tests verify expected output formats\n\n## Files Likely Involved\n- `tests/contracts/__init__.py` (new)\n- `tests/contracts/test_harness_contracts.py` (new)\n- `tests/conftest.py` (register marker)\n- `pyproject.toml` (marker config)\n\n## Notes\nContract tests should be fast and not require API keys. They only verify CLI availability and output format.", "status": "open", "priority": 1, "issue_type": "task", "labels": ["phase-2", "model:sonnet", "complexity:medium", "test", "setup"], "dependencies": [{"depends_on_id": "cub-E02", "type": "parent-child"}]}
{"id": "cub-010", "title": "Enable parallel test execution with pytest-xdist", "description": "## Context\nWith more tests, CI time increases. pytest-xdist enables parallel test execution for faster feedback.\n\n## Implementation Hints\n\n**Recommended Model:** haiku\n**Estimated Duration:** 1-2 hours\n**Approach:** Add pytest-xdist to dev dependencies, update CI to use -n auto.\n\n## Implementation Steps\n1. Add pytest-xdist to pyproject.toml dev dependencies\n2. Test locally with `pytest -n auto`\n3. Verify tests don't have race conditions\n4. Update .github/workflows/test.yml to use -n auto\n5. Ensure coverage still works with xdist\n6. Document in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] pytest-xdist in dev dependencies\n- [ ] `pytest -n auto` works locally\n- [ ] CI uses parallel execution\n- [ ] Coverage reporting still works\n- [ ] No test race conditions\n\n## Files Likely Involved\n- `pyproject.toml`\n- `.github/workflows/test.yml`\n- `CLAUDE.md`\n\n## Notes\nSome tests may need fixtures modified for isolation. Watch for shared state issues.", "status": "open", "priority": 1, "issue_type": "task", "labels": ["phase-2", "model:haiku", "complexity:low", "setup", "checkpoint"], "dependencies": [{"depends_on_id": "cub-E02", "type": "parent-child"}, {"depends_on_id": "cub-006", "type": "blocks"}, {"depends_on_id": "cub-007", "type": "blocks"}, {"depends_on_id": "cub-008", "type": "blocks"}]}
{"id": "cub-E03", "title": "Phase 3: Smart Local Testing", "description": "## Context\nRunning the full test suite on every change is slow. Smart test selection runs only affected tests for faster local feedback.\n\n## Goal\nAdd pytest-testmon, create pre-commit hook, document usage.\n\n## Success Criteria\n- [ ] pytest-testmon installed and working\n- [ ] Pre-commit hook runs affected tests\n- [ ] make test-fast target available\n- [ ] Usage documented in CLAUDE.md", "status": "open", "priority": 2, "issue_type": "epic", "labels": ["phase-3", "model:sonnet", "complexity:medium"], "dependencies": [{"depends_on_id": "cub-E02", "type": "blocks"}]}
{"id": "cub-011", "title": "Add pytest-testmon for smart test selection", "description": "## Context\npytest-testmon tracks file dependencies and only runs tests affected by code changes. This dramatically speeds up local iteration.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-3 hours\n**Approach:** Add pytest-testmon, configure it, verify it works with cub's test structure.\n\n## Implementation Steps\n1. Add pytest-testmon to pyproject.toml dev dependencies\n2. Run `pytest --testmon` to build initial database\n3. Make a small code change and verify only affected tests run\n4. Configure testmon settings if needed\n5. Add .testmondata to .gitignore\n6. Document usage in CLAUDE.md: `pytest --testmon`\n\n## Acceptance Criteria\n- [ ] pytest-testmon in dev dependencies\n- [ ] `pytest --testmon` works correctly\n- [ ] Only affected tests run on code changes\n- [ ] .testmondata in .gitignore\n- [ ] Usage documented\n\n## Files Likely Involved\n- `pyproject.toml`\n- `.gitignore`\n- `CLAUDE.md`\n\n## Notes\ntestmon database needs to be rebuilt after major refactors or dependency changes.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-3", "model:sonnet", "complexity:medium", "setup"], "dependencies": [{"depends_on_id": "cub-E03", "type": "parent-child"}]}
{"id": "cub-012", "title": "Create pre-commit hook for affected tests", "description": "## Context\nA pre-commit hook prevents committing code that breaks tests without having to remember to run them manually.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-3 hours\n**Approach:** Create a hook that uses testmon to run only affected tests before commit.\n\n## Implementation Steps\n1. Create .pre-commit-config.yaml if not exists\n2. Add local hook for pytest --testmon\n3. Configure hook to fail on test failures\n4. Test hook works on commit\n5. Document setup in CLAUDE.md\n6. Consider making hook optional (can be skipped)\n\n## Acceptance Criteria\n- [ ] Pre-commit hook exists\n- [ ] Hook runs affected tests on commit\n- [ ] Hook fails commit if tests fail\n- [ ] Hook can be skipped if needed (--no-verify)\n- [ ] Setup documented\n\n## Files Likely Involved\n- `.pre-commit-config.yaml`\n- `CLAUDE.md`\n\n## Notes\nPre-commit hooks should be fast. testmon helps but may still be slow on large changes.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-3", "model:sonnet", "complexity:medium", "setup"], "dependencies": [{"depends_on_id": "cub-E03", "type": "parent-child"}, {"depends_on_id": "cub-011", "type": "blocks"}]}
{"id": "cub-013", "title": "Add make test-fast target for quick iteration", "description": "## Context\nA simple command for quick test runs makes the testing workflow more accessible.\n\n## Implementation Hints\n\n**Recommended Model:** haiku\n**Estimated Duration:** 30 minutes - 1 hour\n**Approach:** Add Makefile targets for common test commands.\n\n## Implementation Steps\n1. Create or update Makefile\n2. Add `test-fast` target using pytest --testmon\n3. Add `test` target for full test run\n4. Add `test-cov` target for coverage report\n5. Document in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] `make test-fast` runs affected tests\n- [ ] `make test` runs full test suite\n- [ ] `make test-cov` generates coverage report\n- [ ] Targets work correctly\n- [ ] Usage documented\n\n## Files Likely Involved\n- `Makefile` (new or update)\n- `CLAUDE.md`\n\n## Notes\nKeep Makefile simple. Avoid complex shell logic.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-3", "model:haiku", "complexity:low", "setup", "checkpoint"], "dependencies": [{"depends_on_id": "cub-E03", "type": "parent-child"}, {"depends_on_id": "cub-011", "type": "blocks"}]}
{"id": "cub-E04", "title": "Phase 4: CI Hardening", "description": "## Context\nCI should be optimized for different use cases: fast feedback on push, thorough checks on PR, weekly contract validation.\n\n## Goal\nSplit CI into separate jobs, add coverage trend gate, add badges to README.\n\n## Success Criteria\n- [ ] Separate CI jobs for unit/integration/contracts\n- [ ] Coverage trend gate prevents regressions\n- [ ] README has test status badge\n- [ ] Weekly contract test job runs", "status": "open", "priority": 2, "issue_type": "epic", "labels": ["phase-4", "model:sonnet", "complexity:medium"], "dependencies": [{"depends_on_id": "cub-E03", "type": "blocks"}]}
{"id": "cub-014", "title": "Split CI workflow into separate jobs", "description": "## Context\nDifferent test types have different purposes and should run at different times. Fast unit tests on every push, thorough tests on PRs.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 3-4 hours\n**Approach:** Modify .github/workflows/test.yml to have multiple jobs with different triggers.\n\n## Implementation Steps\n1. Analyze current test.yml workflow\n2. Create separate jobs:\n   - `unit`: Fast tests, runs on push and PR\n   - `integration`: Slow tests, runs on PR merge to main\n   - `contracts`: Weekly schedule, verifies CLI compatibility\n3. Configure proper triggers for each job\n4. Ensure coverage is reported from unit job\n5. Add job dependencies where needed\n6. Test workflow changes in a branch\n\n## Acceptance Criteria\n- [ ] Unit tests run on every push\n- [ ] Integration tests run on PR merge\n- [ ] Contract tests run weekly\n- [ ] Coverage still uploads to Codecov\n- [ ] Workflow is documented\n\n## Files Likely Involved\n- `.github/workflows/test.yml`\n- `.github/workflows/contracts.yml` (possibly new)\n\n## Notes\nBe careful with workflow syntax. Test in a branch before merging.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-4", "model:sonnet", "complexity:medium", "setup"], "dependencies": [{"depends_on_id": "cub-E04", "type": "parent-child"}]}
{"id": "cub-015", "title": "Add coverage trend gate to CI", "description": "## Context\nPreventing coverage regression ensures test quality doesn't degrade over time.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 2-3 hours\n**Approach:** Configure Codecov to fail PRs if coverage drops, or add a custom coverage comparison step.\n\n## Implementation Steps\n1. Research Codecov PR comment configuration\n2. Configure codecov.yml for coverage thresholds\n3. Set up coverage diff threshold (e.g., no more than 1% drop)\n4. Add fail condition for coverage regression\n5. Test with a PR that decreases coverage\n6. Document policy in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] Codecov configured with thresholds\n- [ ] PRs fail if coverage drops significantly\n- [ ] Coverage diff shown in PR comments\n- [ ] Policy documented\n\n## Files Likely Involved\n- `codecov.yml` (new or update)\n- `.github/workflows/test.yml`\n- `CLAUDE.md`\n\n## Notes\nBe reasonable with thresholds. Allow small fluctuations to avoid blocking legitimate changes.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-4", "model:sonnet", "complexity:medium", "setup"], "dependencies": [{"depends_on_id": "cub-E04", "type": "parent-child"}, {"depends_on_id": "cub-014", "type": "blocks"}]}
{"id": "cub-016", "title": "Add test status badge to README", "description": "## Context\nBadges provide quick visibility into project health.\n\n## Implementation Hints\n\n**Recommended Model:** haiku\n**Estimated Duration:** 30 minutes\n**Approach:** Add GitHub Actions and Codecov badges to README.\n\n## Implementation Steps\n1. Get GitHub Actions badge URL\n2. Get Codecov badge URL\n3. Add badges to README.md near the title\n4. Verify badges render correctly\n5. Consider adding other relevant badges\n\n## Acceptance Criteria\n- [ ] CI status badge in README\n- [ ] Coverage badge in README\n- [ ] Badges render correctly\n- [ ] Badges link to relevant pages\n\n## Files Likely Involved\n- `README.md`\n\n## Notes\nSimple task. Just add markdown badges.", "status": "open", "priority": 2, "issue_type": "task", "labels": ["phase-4", "model:haiku", "complexity:low", "docs", "checkpoint"], "dependencies": [{"depends_on_id": "cub-E04", "type": "parent-child"}, {"depends_on_id": "cub-015", "type": "blocks"}]}
{"id": "cub-E05", "title": "Phase 5: Refinement - Advanced Testing Features", "description": "## Context\nWith solid foundation in place, add advanced testing features for higher confidence.\n\n## Goal\nAdd property-based testing with Hypothesis, Docker integration tests for sandbox, performance tests.\n\n## Success Criteria\n- [ ] Hypothesis tests for parsing logic\n- [ ] Docker integration tests for sandbox module\n- [ ] Performance regression tests for critical paths", "status": "open", "priority": 3, "issue_type": "epic", "labels": ["phase-5", "model:sonnet", "complexity:high"], "dependencies": [{"depends_on_id": "cub-E04", "type": "blocks"}]}
{"id": "cub-017", "title": "Add Hypothesis property-based tests for parsing logic", "description": "## Context\nProperty-based testing generates many random inputs to find edge cases human-written tests miss. Useful for parsers.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 4-6 hours\n**Approach:** Add hypothesis, write property tests for config parsing, markdown parsing, and JSON parsing.\n\n## Implementation Steps\n1. Add hypothesis to pyproject.toml dev dependencies\n2. Create tests/property/ directory\n3. Write property tests for config loading (valid configs should load)\n4. Write property tests for markdown parsing (shouldn't crash on random input)\n5. Write property tests for JSON parsing (beads format)\n6. Configure hypothesis settings appropriately\n7. Document in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] Hypothesis in dev dependencies\n- [ ] Property tests for config parsing\n- [ ] Property tests for markdown parsing\n- [ ] Property tests for JSON/JSONL parsing\n- [ ] Tests don't flake or timeout\n- [ ] Usage documented\n\n## Files Likely Involved\n- `pyproject.toml`\n- `tests/property/__init__.py` (new)\n- `tests/property/test_config_properties.py` (new)\n- `tests/property/test_parsing_properties.py` (new)\n- `CLAUDE.md`\n\n## Notes\nProperty tests can be slow. Configure reasonable limits. Use @settings to control duration.", "status": "open", "priority": 3, "issue_type": "task", "labels": ["phase-5", "model:sonnet", "complexity:high", "test"], "dependencies": [{"depends_on_id": "cub-E05", "type": "parent-child"}]}
{"id": "cub-018", "title": "Add Docker integration tests for sandbox module", "description": "## Context\nThe sandbox module manages Docker containers. Integration tests verify the full lifecycle works.\n\n## Implementation Hints\n\n**Recommended Model:** opus\n**Estimated Duration:** 4-6 hours\n**Approach:** Create integration tests that spin up real Docker containers, run commands, and verify results.\n\n## Implementation Steps\n1. Create tests/integration/test_sandbox_integration.py\n2. Add pytest marker @pytest.mark.integration\n3. Test sandbox creation and destruction\n4. Test command execution in sandbox\n5. Test file mounting\n6. Test cleanup on failure\n7. Mark tests to skip if Docker not available\n8. Configure CI to run these tests separately\n\n## Acceptance Criteria\n- [ ] Integration tests for sandbox lifecycle\n- [ ] Tests skip gracefully without Docker\n- [ ] Tests clean up containers after running\n- [ ] CI runs these tests on merge (not on every push)\n- [ ] Tests are isolated and don't conflict\n\n## Files Likely Involved\n- `tests/integration/__init__.py` (new)\n- `tests/integration/test_sandbox_integration.py` (new)\n- `.github/workflows/test.yml`\n- `tests/conftest.py`\n\n## Notes\nDocker tests need careful cleanup. Use pytest fixtures with finalizers to ensure containers are removed.", "status": "open", "priority": 3, "issue_type": "task", "labels": ["phase-5", "model:opus", "complexity:high", "test"], "dependencies": [{"depends_on_id": "cub-E05", "type": "parent-child"}]}
{"id": "cub-019", "title": "Add performance regression tests for critical paths", "description": "## Context\nPerformance regressions can creep in unnoticed. Benchmarks catch them early.\n\n## Implementation Hints\n\n**Recommended Model:** sonnet\n**Estimated Duration:** 3-4 hours\n**Approach:** Use pytest-benchmark to track performance of critical operations.\n\n## Implementation Steps\n1. Add pytest-benchmark to dev dependencies\n2. Create tests/benchmarks/ directory\n3. Benchmark config loading\n4. Benchmark task selection algorithm\n5. Benchmark prompt generation\n6. Store baseline results\n7. Configure CI to compare against baseline\n8. Document in CLAUDE.md\n\n## Acceptance Criteria\n- [ ] pytest-benchmark in dev dependencies\n- [ ] Benchmarks for critical paths\n- [ ] Baseline results stored\n- [ ] CI compares against baseline\n- [ ] Significant regressions fail CI\n\n## Files Likely Involved\n- `pyproject.toml`\n- `tests/benchmarks/__init__.py` (new)\n- `tests/benchmarks/test_performance.py` (new)\n- `.github/workflows/test.yml`\n\n## Notes\nBenchmarks should be stable. Avoid measuring I/O or network. Focus on CPU-bound logic.", "status": "open", "priority": 3, "issue_type": "task", "labels": ["phase-5", "model:sonnet", "complexity:medium", "test", "checkpoint"], "dependencies": [{"depends_on_id": "cub-E05", "type": "parent-child"}, {"depends_on_id": "cub-017", "type": "blocks"}, {"depends_on_id": "cub-018", "type": "blocks"}]}
