# Project Prompt

This file provides context for AI coding agents working on this project.

## Overview


Improve test coverage for the Cub CLI to ensure critical paths are tested, CI is trustworthy, and the codebase can evolve quickly with confidence. The approach should be realistic about what can be validated via tests vs. what remains "less proven," with this confidence level tracked and surfaced to AI coding agents working on the codebase.

## Problem Statement

## Problem Statement


Cub has 54% overall test coverage (779 pytest tests, ~496 BATS tests), but critical execution paths like `cli/run.py` (the core loop) have only 14% coverage. This creates risk when evolving the codebase - developers and AI agents can't trust that changes won't break existing functionality. The concern is not just hitting coverage numbers, but ensuring the tests reflect reality and provide genuine confidence.

## Refined Vision

## Technical Approach


This architecture establishes a layered testing strategy for Cub that provides high confidence in critical paths while remaining maintainable as the codebase evolves. The design introduces stability tiers that communicate module confidence levels to both human developers and AI agents, with coverage thresholds enforced per tier.

The core insight is that not all code needs the same testing rigor. Core abstractions (config, task backend, harness backend) are "solid" and require 80%+ coverage. The main execution loop and primary implementations are "moderate" at 60%+. Newer features are "experimental" at 40%+. UI-heavy and bash-delegated code has no threshold but is covered by BATS tests.

For external dependencies—especially AI harnesses—we use a three-layer approach: unit tests with mocks for fast feedback, contract tests to catch breaking changes in CLI interfaces, and optional integration tests for full end-to-end verification.

## Technology Stack

| Layer | Choice | Rationale |

## Architecture


```
┌─────────────────────────────────────────────────────────────────┐
│                         TEST PYRAMID                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │              INTEGRATION TESTS                          │   │
│   │   Real subprocess calls in isolated environments        │   │
│   │   Run: CI only (slow), skip by default locally          │   │
│   │   Location: tests/integration/                          │   │
│   └─────────────────────────────────────────────────────────┘   │
│                            ▲                                    │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │              CONTRACT TESTS                             │   │
│   │   Verify external tool behavior matches expectations    │   │
│   │   Run: Weekly CI job (catch upstream changes)           │   │
│   │   Location: tests/contracts/                            │   │
│   └─────────────────────────────────────────────────────────┘   │
│                            ▲                                    │

## Requirements

### P0 (Must Have)

- **Critical path coverage**: Test the core execution loop (`cli/run.py`) with meaningful tests that exercise real logic, not just mocked shells
- **Layered test strategy**: Unit tests for pure logic, integration tests for external dependency interactions (git, harnesses, Docker)
- **Stability tracking**: Create `.cub/STABILITY.md` that documents per-module confidence levels, referenced by CLAUDE.md/AGENTS.md
- **CI reliability**: Ensure test suite runs consistently without flaky failures that erode trust

## Constraints


- Use existing pytest framework (no new test frameworks)
- Must work with current GitHub Actions infrastructure
- Tests must be maintainable as code evolves - avoid over-mocking that creates fragile tests

---

Generated by cub prep. Session artifacts in .cub/sessions/
