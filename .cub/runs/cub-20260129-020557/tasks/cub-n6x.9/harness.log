

I'll start by claiming the task and exploring the codebase to understand the headline rendering logic.Now let me read the key files to understand the exact code I need to modify:Now let me check how `_render_run_event` gets `harness_name` and if model info is available:Now let me check where the RunEvent has model info and how the event is constructed for TASK_SELECTED:Now I have a clear picture. The `display_task_info` function at line 408 needs to show harness and model info. Let me check what info is available in the call context at line 1436, and find existing tests:Now I understand the full picture. Let me implement the changes:

1. Modify `display_task_info` to accept optional `harness_name` and `model` params
2. Update the call site in `_render_run_event` to pass them
3. Update testsNow update the call site to pass harness_name and model. I need to determine the model at the call site. The model comes from `config.harness.model` or the task's `model_label`. Let me check what `config` has:The `config` in `_render_run_event` is a `CubConfig`. Let me check how model is resolved in the run loop - the `RunConfig` has `model` and `harness_name`. Let me look at how these are available in the render event context:Now let me check where `run_config` is created to understand model availability:Good. So at the `_render_run_event` level, `harness_name` is already available as a parameter. For model, I need to check what model info is available. The `config` (CubConfig) is there, but the CLI `model` arg was used to build `run_config`. Let me check if there's a `model` variable in scope at the event rendering:The `model` variable is available in the `run` function scope but not passed to `_render_run_event`. The best approach is to determine the model at the TASK_SELECTED event site. In `_render_run_event`, I have `config` (CubConfig) and I can get the task's `model_label`. But the CLI `model` override isn't available there.

Let me check what CubConfig has for model:Good. `config.harness.model` exists. Now let me look at where the CLI `model` arg is passed - in `build_run_config`:So the model is resolved as `model or cfg.harness.model` in `run_config`. But in `_render_run_event`, I don't have `run_config` - I have `config` (CubConfig) and `harness_name`. Let me check if the model variable is accessible or if I should pass it:

The cleanest approach is to resolve the model at the call site in `_render_run_event`. I have the task (with `model_label`) and the `config` (CubConfig with `config.harness.model`). But the CLI `model` override isn't directly available. 

Looking at the pattern from the loop code: `self.config.model or task.model_label` where `self.config` is `RunConfig`. The `_render_run_event` doesn't have `RunConfig` available.

The best approach: add a `model` parameter to `_render_run_event` so the caller passes it. Let me see where `_render_run_event` is called:I see that `run_config` is available in the same scope as the `_render_run_event` call. I'll add a `model` parameter to `_render_run_event` and pass `run_config.model` from the call site. For the task's model_label, I can compute the effective model in the TASK_SELECTED handler.Now update the call site to pass `model`:Now update the TASK_SELECTED handler to pass harness and model info to `display_task_info`:Now let me check if there are other callers of `_render_run_event` that need updating:Good, only one call site. Now let me also check if `display_task_info` is called from anywhere else besides `_render_run_event`:

Already checked - there are only 2 references: the definition and the one call in `_render_run_event`. The tests call it directly.

Now update the tests:Now let's run the tests and type checking:All 6 tests pass. Now run broader checks:The mypy error is pre-existing (line 292, unrelated to my changes). Let me verify:Let me run the full test suite quickly to make sure nothing else is broken:The failure is in `test_harness_claude_sdk.py` - an environment variable test that's a pre-existing issue (CUB_RUN_ACTIVE env var leak from the test environment). Not related to my changes.

Let me confirm my specific test file passes and then close the task:Now let me check if all tasks are closed:There are still open tasks, so I won't output the COMPLETE signal. The task `cub-n6x.9` has been completed successfully.

## Summary

**What I did:**

1. **Modified `display_task_info()`** in `src/cub/cli/run.py` to accept optional `harness_name` and `model` keyword arguments. When provided and non-empty, they're added as additional rows ("Harness" and "Model") in the task info panel. Empty/None values are gracefully omitted.

2. **Updated `_render_run_event()`** to accept a `model` parameter and pass both `harness_name` and the resolved effective model to `display_task_info()`. The model is resolved using the same priority as the run loop: CLI/config override takes precedence over the task's `model_label`.

3. **Updated the call site** in the `run()` function to pass `run_config.model` to `_render_run_event()`.

4. **Added 4 new tests** covering:
   - Both harness and model displayed when provided
   - Neither shown when not provided (default behavior)
   - Only harness shown when model is not configured
   - Empty strings treated as unconfigured (no "Harness" or "Model" rows)