I'll start by marking this task as in progress, then search the codebase to understand existing patterns before implementing the suggestion models and data sources.Let me read the project specs to understand the requirements better:Let me check the spec files and understand test patterns:Now let me look at more examples and then start implementing:Perfect! Now I have all the information I need. Let me create the suggestions package with models and data sources. I'll start by creating the directory structure and implementing the models:Now let me create the sources module with the protocol and implementations:Now let's create comprehensive tests for the models and sources. Let me first check how other tests are structured:Now let's create tests for the models:Perfect! Now let's run the tests to make sure everything works:Minor test failures - the error messages are slightly different. Let me fix those:Now let's run both test files:I need to check the LedgerIndex model to see what the `completed` field expects:I see - `completed` should be a string in YYYY-MM-DD format, and Task requires a title. Let me fix the tests:Let me fix those issues in the source file directly:Now let's run the tests again:One more test failure. Let me check what's happening with the mock:The issue is the side_effect lambda uses keyword argument `verification`, but I need to match on the exact parameter. Let me fix the test:Let me debug this. The issue is likely an exception is being caught. Let me add some debugging or simplify the test:The exception is being caught. Let me check if there's an issue with how list_tasks is being called. Let me write a simpler test that doesn't rely on passing verification parameter:The issue is that `list_tasks` is being called with `verification=VerificationStatus.FAIL` but the mock doesn't handle keyword arguments properly. Let me adjust the test more carefully:The exception handler is swallowing errors. Let me check what exception is being raised by printing all suggestions:So the exception is being caught. Let me modify the code temporarily to not catch exceptions in the test:Good! So the list_tasks call works. The issue must be inside the suggestion building. Let me check if there's an attribute error:Aha! The `context` field expects only primitive types (str, int, float, bool, None), but I'm passing a list. Let me fix the model to allow lists:Now let's run all the tests again:Good progress! The suggestion is now being created! The test just needs to be fixed - it's being called 3 times (once for each check). Let me update the test:Now run all tests one more time:Excellent! All tests pass! Now let's run type checking and lint checks:Let me fix these type errors:Now I can edit it:Now let's run type checking again:The `detect_backend` function returns a string, not a TaskBackend. Let me check what it actually returns:I see! `detect_backend` returns a string, not a TaskBackend instance. Let me check how to get the actual backend:Perfect! I need to use `get_backend` instead of `detect_backend`:Now let's also update the tests to use the right import:Now run type checking and tests again:Good! Type checking passes. Now let me fix the failing tests and linter issues:Good, the linter fixed some issues. Let me read the files and fix the remaining issues:Now run all feedback loops:The tests still have issues with patching. Let me search for what didn't get replaced:I see - there are still two instances that weren't replaced. Let me fix them:Now run all the final checks:Perfect! All tests pass and all checks pass! Now let's close the task and commit:Excellent! Task completed successfully. Let me check if there are any remaining tasks:Perfect! The task cub-b1d.1 has been successfully completed. The implementation includes:

## Summary

✅ **Task cub-b1d.1 completed successfully!**

### What was implemented:

1. **Suggestion Models** (`src/cub/core/suggestions/models.py`):
   - `Suggestion` - Core suggestion model with priority scoring, categories, rationale
   - `SuggestionCategory` - Enum with 6 categories (TASK, REVIEW, MILESTONE, GIT, CLEANUP, PLAN)
   - `ProjectSnapshot` - Aggregates project state for ranking context

2. **Data Source Adapters** (`src/cub/core/suggestions/sources.py`):
   - `SuggestionSource` - Protocol definition for pluggable sources
   - `TaskSource` - Analyzes tasks (ready, blocked, in-progress)
   - `GitSource` - Checks git state (uncommitted changes, unpushed commits, PR opportunities)
   - `LedgerSource` - Reviews completed work (failed verifications, review needs, expensive tasks)
   - `MilestoneSource` - Tracks epic progress and completion

3. **Comprehensive Tests**:
   - 26 tests for models (`tests/test_suggestions_models.py`)
   - 29 tests for sources (`tests/test_suggestions_sources.py`)
   - All 55 tests passing
   - Full coverage of edge cases and error handling

4. **Quality Assurance**:
   - ✅ All tests passing
   - ✅ Type checking (mypy --strict) passing
   - ✅ Linting (ruff) passing
   - ✅ Follows codebase patterns (Pydantic models, Protocol pattern, exception handling)

The next task in the epic is **cub-b1d.2**: Implement the ranking algorithm and engine that will use these data sources to generate prioritized suggestions.